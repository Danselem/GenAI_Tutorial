{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0076e046",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "# Define the project identifier\n",
    "project_identifier = \"calculus-code-agent\"\n",
    "\n",
    "# Export all the top level spans \n",
    "spansdf = Client().spans.get_spans_dataframe( project_identifier=project_identifier)\n",
    "\n",
    "spansdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb43058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the question and generated code from the spans\n",
    "query = SpanQuery().where(\"name == 'write_calculus_code'\")\n",
    "spansdf = Client().spans.get_spans_dataframe(query=query, project_identifier=project_identifier)\n",
    "\n",
    "# drop any traces not from our dataset\n",
    "spansdf.dropna(\n",
    "    subset=[\"attributes.metadata\"], inplace=True\n",
    ")\n",
    "\n",
    "spansdf[\"question\"] = spansdf[\"attributes.input.value\"] \n",
    "spansdf[\"code\"] = spansdf[\"attributes.output.value\"]\n",
    "\n",
    "# Now get the code execution result spans\n",
    "query = SpanQuery().where(\"name == 'execute_python_code'\")\n",
    "code_res_df = Client().spans.get_spans_dataframe(\n",
    "    query=query, project_identifier=project_identifier\n",
    ")\n",
    "\n",
    "code_res_df[\"result\"] = code_res_df[\"attributes.output.value\"]\n",
    "\n",
    "# Now get the answer generation spans\n",
    "query = SpanQuery().where(\"name == 'generate_answer'\")\n",
    "gen_df = Client().spans.get_spans_dataframe(\n",
    "    query=query, project_identifier=project_identifier\n",
    ")\n",
    "\n",
    "gen_df[\"answer\"] = gen_df[\"attributes.output.value\"]\n",
    "\n",
    "\n",
    "# Combine the spans together \n",
    "spans_df = spansdf.merge(\n",
    "    code_res_df[[\"context.trace_id\", \"result\",]],\n",
    "    on=\"context.trace_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "spans_df = spans_df.merge(\n",
    "    gen_df[[\"context.trace_id\", \"answer\",]],\n",
    "    on=\"context.trace_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "spans_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472308f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    GoogleGenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API key will be read from environment\n",
    "model = GoogleGenAIModel(model=\"gemini-2.0-flash-001\", default_concurrency=1, initial_rate_limit=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HALLUCINATION_PROMPT_TEMPLATE = \"\"\"\n",
    "In this task, you will be given a query, code generated to address the query, \n",
    "the execution result of that code, and an answer produced from them. \n",
    "Your job is to check if the answer is faithful to the provided code and result.\n",
    "\n",
    "- \"factual\" means the answer accurately reflects the information in the code and result.  \n",
    "- \"hallucinated\" means the answer introduces details not supported by the code and result, \n",
    "  or contradicts them.  \n",
    "\n",
    "Output exactly one word: either \"factual\" or \"hallucinated\". Do not output anything else.\n",
    "\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {question}\n",
    "************\n",
    "[Generated code]: {code}\n",
    "************\n",
    "[Generated result]: {result}\n",
    "************\n",
    "[Answer]: {answer}\n",
    "************\n",
    "[END DATA]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
    "hallucination_classifications = llm_classify(\n",
    "    data=spans_df, \n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE, \n",
    "    model=model, \n",
    "    rails=rails,\n",
    "    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_classifications.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e904a6d",
   "metadata": {},
   "source": [
    "## Checking for Code Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe842e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import CODE_READABILITY_PROMPT_RAILS_MAP, TOOL_CALLING_PROMPT_RAILS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_READABILITY_PROMPT_TEMPLATE = \"\"\"\n",
    "    You will be given a question and a piece of code. \n",
    "    Your task is to judge whether the code is easy to read and understand.\n",
    "\n",
    "    - \"readable\" means the code is clear, structured, and understandable.  \n",
    "    - \"unreadable\" means the code is messy, confusing, or difficult to follow.  \n",
    "\n",
    "    Output exactly one word: either \"readable\" or \"unreadable\". Do not output anything else.\n",
    "\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Code]: {code}\n",
    "    ************\n",
    "    [END DATA]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_rails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\n",
    "readability_classifications = llm_classify(\n",
    "    data=spans_df, \n",
    "    template=CODE_READABILITY_PROMPT_TEMPLATE, \n",
    "    model=model, \n",
    "    rails=code_rails,\n",
    "    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_classifications.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65a9a8",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e277980",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_CALLING_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given a question, code executed by the tool, and the result of that execution. \n",
    "Your task is to decide whether the tool call (code + result) correctly answers the question.\n",
    "\n",
    "- \"correct\" means the code was appropriate for the question, the execution result is consistent with it, \n",
    "  and no extra information outside the question was introduced.  \n",
    "- \"incorrect\" means the code does not match the question, produces a result unrelated to the question, \n",
    "  or introduces information not present in the question.  \n",
    "\n",
    "Output exactly one word: either \"correct\" or \"incorrect\". Do not output anything else.\n",
    "\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {question}\n",
    "************\n",
    "[Code Executed]: {code}\n",
    "************\n",
    "[Result]: {result}\n",
    "************\n",
    "[END DATA]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85493d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_rails = list(TOOL_CALLING_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "tool_call_evaluations = llm_classify(\n",
    "    data=spans_df, \n",
    "    template=TOOL_CALLING_PROMPT_TEMPLATE, \n",
    "    model=model, \n",
    "    rails=tool_rails,\n",
    "    provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f47c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_evaluations.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54dce2",
   "metadata": {},
   "source": [
    "## Logging the Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Client().spans.log_span_annotations_dataframe(\n",
    "    dataframe=hallucination_classifications,\n",
    "    annotation_name=\"Hallucination\",\n",
    "    annotator_kind=\"LLM\",\n",
    ")\n",
    "Client().spans.log_document_annotations_dataframe(\n",
    "    dataframe=readability_classifications,\n",
    "    annotation_name=\"Code Readability\",\n",
    "    annotator_kind=\"LLM\",\n",
    ")\n",
    "Client().spans.log_span_annotations_dataframe(\n",
    "    dataframe=tool_call_evaluations,\n",
    "    annotation_name=\"Tool Call Correctness\",\n",
    "    annotator_kind=\"LLM\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI_Tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
